(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{570:function(s,t,a){s.exports=a.p+"assets/img/1.e2954a38.png"},591:function(s,t,a){"use strict";a.r(t);var n=a(3),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,n=s._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[n("h2",{attrs:{id:"_1-scrapy介绍"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-scrapy介绍"}},[s._v("#")]),s._v(" 1.scrapy介绍")]),s._v(" "),n("p",[n("img",{attrs:{src:a(570),alt:""}})]),s._v(" "),n("h2",{attrs:{id:"_2-安装scrapy框架"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-安装scrapy框架"}},[s._v("#")]),s._v(" 2.安装scrapy框架:")]),s._v(" "),n("p",[s._v("进入虚拟环境后,通过命令")]),s._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[s._v("pip "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("install")]),s._v(" scrapy\n")])])]),n("p",[s._v("来安装scrapy框架("),n("strong",[s._v("如果在在windows下,还需要安装")]),n("code",[s._v("pypiwin32")]),s._v(")")]),s._v(" "),n("h2",{attrs:{id:"_3-创建项目和爬虫"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-创建项目和爬虫"}},[s._v("#")]),s._v(" 3.创建项目和爬虫:")]),s._v(" "),n("p",[s._v("1.通过以下命令来创建scrapy项目")]),s._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[s._v("scrapy startproject "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("项目名称"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),n("p",[s._v("2.进入scrapy项目路径后,通过以下命令来创建scrapy爬虫")]),s._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[s._v("scrapy genspider "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("爬虫名称"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"爬虫的域名"')]),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),n("p",[s._v("创建带模板的scrapy爬虫")]),s._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[s._v("scrapy genspider -t "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("模板名称"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("爬虫名称"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"爬虫的域名"')]),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),n("p",[n("strong",[s._v("(爬虫的名称不能和项目名称一样,且爬虫名称唯一")]),s._v(")")]),s._v(" "),n("h2",{attrs:{id:"_4-启动爬虫"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-启动爬虫"}},[s._v("#")]),s._v(" 4.启动爬虫")]),s._v(" "),n("p",[s._v("通过以下命令来启动爬虫")]),s._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[s._v("scrapy crawl "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("爬虫名称"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),n("p",[s._v("可以建立"),n("code",[s._v("start.py")]),s._v("文件,来通过启动该文件快速启动爬虫")]),s._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" cmdline\ncmdline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("execute"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"scrapy"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"crawl"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"<爬虫名称>"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),n("h2",{attrs:{id:"_5-项目目录结构"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_5-项目目录结构"}},[s._v("#")]),s._v(" 5.项目目录结构")]),s._v(" "),n("ol",[n("li",[n("code",[s._v("items.py")]),s._v(":用来存放爬虫爬下来的数据模型")]),s._v(" "),n("li",[n("code",[s._v("middlewares.py")]),s._v(":用来存放各种中间件的文件.")]),s._v(" "),n("li",[n("code",[s._v("pipelines.py")]),s._v(":用来将items的模型存储到本地磁盘中.")]),s._v(" "),n("li",[n("code",[s._v("settings.py")]),s._v(":该爬虫的一些配置信息(比如:请求头,多久发送一次请求,ip代理池等)")]),s._v(" "),n("li",[n("code",[s._v("scrapy.cfg")]),s._v(":项目的配置信息")]),s._v(" "),n("li",[n("code",[s._v("spiders")]),s._v("文件夹:爬虫存放路径")])]),s._v(" "),n("h2",{attrs:{id:"糗事百科爬虫笔记"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#糗事百科爬虫笔记"}},[s._v("#")]),s._v(" 糗事百科爬虫笔记")]),s._v(" "),n("ol",[n("li",[n("p",[n("strong",[s._v("response是一个"),n("code",[s._v("scrapy.http.response.html.HtmlResponse")]),s._v("对象.可以用"),n("code",[s._v("xpath")]),s._v("和"),n("code",[s._v("css")]),s._v("语法来提取数据.")])])]),s._v(" "),n("li",[n("p",[n("strong",[s._v("提取出来的数据是一个"),n("code",[s._v("Selector")]),s._v("或者一个"),n("code",[s._v("SelectorList")]),s._v("对象,如果想要过去其中的字符串应该执行"),n("code",[s._v("get()")]),s._v("方法或者"),n("code",[s._v("getall()")]),s._v("方法.")])])]),s._v(" "),n("li",[n("p",[n("strong",[n("code",[s._v("getall()")]),s._v(" 方法:获取的是"),n("code",[s._v("Selector")]),s._v("中所有文本.返回的是一个列表.")])])]),s._v(" "),n("li",[n("p",[n("strong",[n("code",[s._v("get()")]),s._v("方法:获取的是"),n("code",[s._v("Selector")]),s._v("中的第一个文本")])])]),s._v(" "),n("li",[n("p",[n("strong",[s._v("如果数据解析回来,要传给pipeline处理,那么可以使用"),n("code",[s._v("yield")]),s._v("来返回.如果不用"),n("code",[s._v("yield")]),s._v("也可以每次将item存入一个列表,最后循环结束后将列表返回")])])]),s._v(" "),n("li",[n("p",[n("strong",[s._v("item建议在items.py中设置好模型,不要使用字典")])])]),s._v(" "),n("li",[n("p",[n("strong",[s._v("pipeline是专门用来保存数据的,其中有三个方法")])]),s._v(" "),n("ul",[n("li",[n("p",[n("code",[s._v("open_spider(self,spider)")]),s._v(":当爬虫被执行时,调用该方法.")])]),s._v(" "),n("li",[n("p",[n("code",[s._v("process_item(self,item,spider)")]),s._v(":当爬虫中有item传过来的时候被调用")])]),s._v(" "),n("li",[n("p",[n("code",[s._v("close_spider(self,spider)")]),s._v(":当爬虫被关闭时,调用该方法.")])])])]),s._v(" "),n("li",[n("p",[n("strong",[s._v("要激活pipeline,应该先在"),n("code",[s._v("settings.py")]),s._v("中设置"),n("code",[s._v("ITEM_PIPELINES")]),s._v(".")])]),s._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[s._v("ITEM_PIPELINES = {\n   'qsbk.pipelines.QsbkPipeline': 300,\n}\n")])])])])]),s._v(" "),n("h2",{attrs:{id:"jsonitemexporter和jsonlinesitemexport"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#jsonitemexporter和jsonlinesitemexport"}},[s._v("#")]),s._v(" JsonItemExporter和JsonLinesItemExport:")]),s._v(" "),n("p",[s._v("保存Json数据的时候可以使用这两个类,让操作更简单.")]),s._v(" "),n("ol",[n("li",[n("p",[n("code",[s._v("JsonItemExporter")]),s._v(":这每次把数据添加到内存中,最后统一讲数据写入到磁盘中.好处是"),n("strong",[s._v("存储的是一个满足json规则的文件")]),s._v(",坏处是"),n("strong",[s._v("比较消耗内存")]),s._v(".示例代码如下:")]),s._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("QsbkPipeline")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fp "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"duanzi.json"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v("'wb'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# fp=filepoint 文件指针,代表操作的那个文件")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# 启动爬虫时,打开一个"duanzi.json"的json文件,方便后续写入操作')]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exporter "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" JsonItemExporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("ensure_ascii"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("encoding"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v("'utf-8'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("start_exporting"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("open_spider")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("spider"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"爬虫开始了"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \n    \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("process_item")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" spider"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("export_item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" item\n    \n    \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("close_spider")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("spider"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("finish_exporting"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("close"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"爬虫结束了"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])])]),s._v(" "),n("li",[n("p",[n("code",[s._v("JsonLinesItemExport")]),s._v(":这个是每次调用"),n("code",[s._v("export_item")]),s._v("的时候就把item写入到磁盘中,好处是"),n("strong",[s._v("每次处理数据的时候就直接存储到了磁盘中,这样对内对消耗较少,数据也比较安全")]),s._v(",坏处是"),n("strong",[s._v("存储的不是一个满足json规则的文件")]),s._v(".示例代码如下:")]),s._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("QsbkPipeline")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fp "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"duanzi.json"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v("'wb'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# fp=filepoint 文件指针,代表操作的那个文件")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# 启动爬虫时,打开一个"duanzi.json"的json文件,方便后续写入操作')]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exporter "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" JsonLinesItemExporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("ensure_ascii"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("encoding"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v("'utf-8'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("open_spider")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("spider"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"爬虫开始了"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \n    \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("process_item")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" spider"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("export_item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" item\n    \n    \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[s._v("close_spider")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("spider"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("close"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[s._v('"爬虫结束了"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])])])]),s._v(" "),n("h2",{attrs:{id:"scrapy下载图片"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#scrapy下载图片"}},[s._v("#")]),s._v(" scrapy下载图片")]),s._v(" "),n("ol",[n("li",[n("p",[s._v("在"),n("code",[s._v("items.py")]),s._v("设置好数据模型,"),n("code",[s._v("image_urls")]),s._v("和"),n("code",[s._v("images")])]),s._v(" "),n("p",[s._v("(***注意:***"),n("code",[s._v("image_urls")]),n("em",[n("strong",[s._v("必须是一个")])]),n("code",[s._v("list")]),s._v(")")]),s._v(" "),n("p",[s._v("(***注意:***"),n("code",[s._v("image_urls")]),n("em",[n("strong",[s._v("必须是一个")])]),n("code",[s._v("list")]),s._v(")")]),s._v(" "),n("p",[s._v("(***注意:***"),n("code",[s._v("image_urls")]),n("em",[n("strong",[s._v("必须是一个")])]),n("code",[s._v("list")]),s._v(")")]),s._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("ImagesItem")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("scrapy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    image_urls "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" scrapy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Field"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    images "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" scrapy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Field"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("pass")]),s._v("\n")])])])]),s._v(" "),n("li",[n("p",[s._v("在"),n("code",[s._v("settings.py")]),s._v("中设置好"),n("code",[s._v("ITEM_PIPELINES")])]),s._v(" "),n("div",{staticClass:"language-py extra-class"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[s._v("ITEM_PIPELINES "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n  "),n("span",{pre:!0,attrs:{class:"token string"}},[s._v("'scrapy.pipelines.images.ImagesPipeline'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])])])]),s._v(" "),n("li",[n("p",[s._v("在"),n("code",[s._v("settings.py")]),s._v("中设置好"),n("code",[s._v("IMAGES_STORE")])]),s._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[s._v("import os\nIMAGES_STORE = os.path.join(os.path.dirname(os.path.dirname(__file__)),'images')\n# 这里使用了os模块,设置图片存放路径为\"../images\"\n")])])])])])])}),[],!1,null,null,null);t.default=e.exports}}]);